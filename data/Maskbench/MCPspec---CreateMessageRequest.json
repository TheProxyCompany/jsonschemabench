{
  "description": "sample MCPspec/CreateMessageRequest.json",
  "meta": {
    "full_size": 6697,
    "stripped_size": 2087,
    "features": [
      "$ref",
      "@minmaxNumber",
      "@siblingKeys",
      "additionalProperties",
      "anyOf",
      "const",
      "enum",
      "format",
      "format:byte",
      "items"
    ],
    "raw_features": [
      "$schema",
      "_boolSchema",
      "definitions",
      "maximum",
      "minimum",
      "properties",
      "required",
      "type",
      "type:array",
      "type:integer",
      "type:number",
      "type:object",
      "type:string"
    ]
  },
  "schema": {
    "$schema": "http://json-schema.org/draft-07/schema#",
    "$ref": "#/definitions/CreateMessageRequest",
    "definitions": {
      "CreateMessageRequest": {
        "description": "A request from the server to sample an LLM via the client. The client has full discretion over which model to select. The client should also inform the user before beginning sampling, to allow them to inspect the request (human in the loop) and decide whether to approve it.",
        "properties": {
          "method": {
            "const": "sampling/createMessage",
            "type": "string"
          },
          "params": {
            "properties": {
              "includeContext": {
                "description": "A request to include context from one or more MCP servers (including the caller), to be attached to the prompt. The client MAY ignore this request.",
                "enum": [
                  "allServers",
                  "none",
                  "thisServer"
                ],
                "type": "string"
              },
              "maxTokens": {
                "description": "The maximum number of tokens to sample, as requested by the server. The client MAY choose to sample fewer tokens than requested.",
                "type": "integer"
              },
              "messages": {
                "items": {
                  "$ref": "#/definitions/SamplingMessage"
                },
                "type": "array"
              },
              "metadata": {
                "additionalProperties": true,
                "description": "Optional metadata to pass through to the LLM provider. The format of this metadata is provider-specific.",
                "properties": {},
                "type": "object"
              },
              "modelPreferences": {
                "$ref": "#/definitions/ModelPreferences",
                "description": "The server's preferences for which model to select. The client MAY ignore these preferences."
              },
              "stopSequences": {
                "items": {
                  "type": "string"
                },
                "type": "array"
              },
              "systemPrompt": {
                "description": "An optional system prompt the server wants to use for sampling. The client MAY modify or omit this prompt.",
                "type": "string"
              },
              "temperature": {
                "type": "number"
              }
            },
            "required": [
              "maxTokens",
              "messages"
            ],
            "type": "object"
          }
        },
        "required": [
          "method",
          "params"
        ],
        "type": "object"
      },
      "SamplingMessage": {
        "description": "Describes a message issued to or received from an LLM API.",
        "properties": {
          "content": {
            "anyOf": [
              {
                "$ref": "#/definitions/TextContent"
              },
              {
                "$ref": "#/definitions/ImageContent"
              }
            ]
          },
          "role": {
            "$ref": "#/definitions/Role"
          }
        },
        "required": [
          "content",
          "role"
        ],
        "type": "object"
      },
      "TextContent": {
        "description": "Text provided to or from an LLM.",
        "properties": {
          "annotations": {
            "properties": {
              "audience": {
                "description": "Describes who the intended customer of this object or data is.\n\nIt can include multiple entries to indicate content useful for multiple audiences (e.g., `[\"user\", \"assistant\"]`).",
                "items": {
                  "$ref": "#/definitions/Role"
                },
                "type": "array"
              },
              "priority": {
                "description": "Describes how important this data is for operating the server.\n\nA value of 1 means \"most important,\" and indicates that the data is\neffectively required, while 0 means \"least important,\" and indicates that\nthe data is entirely optional.",
                "maximum": 1,
                "minimum": 0,
                "type": "number"
              }
            },
            "type": "object"
          },
          "text": {
            "description": "The text content of the message.",
            "type": "string"
          },
          "type": {
            "const": "text",
            "type": "string"
          }
        },
        "required": [
          "text",
          "type"
        ],
        "type": "object"
      },
      "Role": {
        "description": "The sender or recipient of messages and data in a conversation.",
        "enum": [
          "assistant",
          "user"
        ],
        "type": "string"
      },
      "ImageContent": {
        "description": "An image provided to or from an LLM.",
        "properties": {
          "annotations": {
            "properties": {
              "audience": {
                "description": "Describes who the intended customer of this object or data is.\n\nIt can include multiple entries to indicate content useful for multiple audiences (e.g., `[\"user\", \"assistant\"]`).",
                "items": {
                  "$ref": "#/definitions/Role"
                },
                "type": "array"
              },
              "priority": {
                "description": "Describes how important this data is for operating the server.\n\nA value of 1 means \"most important,\" and indicates that the data is\neffectively required, while 0 means \"least important,\" and indicates that\nthe data is entirely optional.",
                "maximum": 1,
                "minimum": 0,
                "type": "number"
              }
            },
            "type": "object"
          },
          "data": {
            "description": "The base64-encoded image data.",
            "format": "byte",
            "type": "string"
          },
          "mimeType": {
            "description": "The MIME type of the image. Different providers may support different image types.",
            "type": "string"
          },
          "type": {
            "const": "image",
            "type": "string"
          }
        },
        "required": [
          "data",
          "mimeType",
          "type"
        ],
        "type": "object"
      },
      "ModelPreferences": {
        "description": "The server's preferences for model selection, requested of the client during sampling.\n\nBecause LLMs can vary along multiple dimensions, choosing the \"best\" model is\nrarely straightforward.  Different models excel in different areasâ€”some are\nfaster but less capable, others are more capable but more expensive, and so\non. This interface allows servers to express their priorities across multiple\ndimensions to help clients make an appropriate selection for their use case.\n\nThese preferences are always advisory. The client MAY ignore them. It is also\nup to the client to decide how to interpret these preferences and how to\nbalance them against other considerations.",
        "properties": {
          "costPriority": {
            "description": "How much to prioritize cost when selecting a model. A value of 0 means cost\nis not important, while a value of 1 means cost is the most important\nfactor.",
            "maximum": 1,
            "minimum": 0,
            "type": "number"
          },
          "hints": {
            "description": "Optional hints to use for model selection.\n\nIf multiple hints are specified, the client MUST evaluate them in order\n(such that the first match is taken).\n\nThe client SHOULD prioritize these hints over the numeric priorities, but\nMAY still use the priorities to select from ambiguous matches.",
            "items": {
              "$ref": "#/definitions/ModelHint"
            },
            "type": "array"
          },
          "intelligencePriority": {
            "description": "How much to prioritize intelligence and capabilities when selecting a\nmodel. A value of 0 means intelligence is not important, while a value of 1\nmeans intelligence is the most important factor.",
            "maximum": 1,
            "minimum": 0,
            "type": "number"
          },
          "speedPriority": {
            "description": "How much to prioritize sampling speed (latency) when selecting a model. A\nvalue of 0 means speed is not important, while a value of 1 means speed is\nthe most important factor.",
            "maximum": 1,
            "minimum": 0,
            "type": "number"
          }
        },
        "type": "object"
      },
      "ModelHint": {
        "description": "Hints to use for model selection.\n\nKeys not declared here are currently left unspecified by the spec and are up\nto the client to interpret.",
        "properties": {
          "name": {
            "description": "A hint for a model name.\n\nThe client SHOULD treat this as a substring of a model name; for example:\n - `claude-3-5-sonnet` should match `claude-3-5-sonnet-20241022`\n - `sonnet` should match `claude-3-5-sonnet-20241022`, `claude-3-sonnet-20240229`, etc.\n - `claude` should match any Claude model\n\nThe client MAY also map the string to a different provider's model name or a different model family, as long as it fills a similar niche; for example:\n - `gemini-1.5-flash` could match `claude-3-haiku-20240307`",
            "type": "string"
          }
        },
        "type": "object"
      }
    }
  },
  "tests": [
    {
      "description": "llama 70b generated positive",
      "valid": true,
      "data": {
        "method": "sampling/createMessage",
        "params": {
          "maxTokens": 1024,
          "messages": [
            {
              "content": {
                "type": "text",
                "text": "Hello, how are you?",
                "annotations": {
                  "priority": 0.5,
                  "audience": [
                    "assistant",
                    "user"
                  ]
                }
              },
              "role": "assistant"
            },
            {
              "content": {
                "type": "text",
                "text": "I'm good, thanks!",
                "annotations": {
                  "priority": 1,
                  "audience": [
                    "assistant"
                  ]
                }
              },
              "role": "user"
            }
          ],
          "modelPreferences": {
            "intelligencePriority": 0.8,
            "costPriority": 0.2,
            "speedPriority": 0.5,
            "hints": [
              {
                "name": "claude-3-5-sonnet"
              }
            ]
          },
          "temperature": 0.7,
          "systemPrompt": "Please respond with a haiku.",
          "stopSequences": [
            "\n",
            "\n\n"
          ],
          "includeContext": "thisServer",
          "metadata": {
            "customKey": "customValue"
          }
        }
      }
    },
    {
      "description": "llama-70b generated negative; focus on anyOf keyword",
      "valid": false,
      "rust_error": "{\"text\":\"Hello, how are you?\",\"annotations\":{\"priority\":0.5,\"audience\":[\"assistant\",\"user\"]}} is not valid under any of the schemas listed in the 'anyOf' keyword",
      "python_error": "{'text': 'Hello, how are you?', 'annotations': {'priority': 0.5, 'audience': ['assistant', 'user']}} is not valid under any of the given schemas\n\nFailed validating 'anyOf' in schema['properties']['params']['properties']['messages']['items']['properties']['content']:\n    {'anyOf': [{'$ref': '#/definitions/TextContent'},\n               {'$ref': '#/definitions/ImageContent'}]}\n\nOn instance['params']['messages'][0]['content']:\n    {'text': 'Hello, how are you?',\n     'annotations': {'priority': 0.5, 'audience': ['assistant', 'user']}}",
      "data": {
        "method": "sampling/createMessage",
        "params": {
          "maxTokens": 1024,
          "messages": [
            {
              "content": {
                "text": "Hello, how are you?",
                "annotations": {
                  "priority": 0.5,
                  "audience": [
                    "assistant",
                    "user"
                  ]
                }
              },
              "role": "assistant"
            },
            {
              "content": {
                "type": "text",
                "text": "I'm good, thanks!",
                "annotations": {
                  "priority": 1,
                  "audience": [
                    "assistant"
                  ]
                }
              },
              "role": "user"
            }
          ],
          "modelPreferences": {
            "intelligencePriority": 0.8,
            "costPriority": 0.2,
            "speedPriority": 0.5,
            "hints": [
              {
                "name": "claude-3-5-sonnet"
              }
            ]
          },
          "temperature": 0.7,
          "systemPrompt": "Please respond with a haiku.",
          "stopSequences": [
            "\n",
            "\n\n"
          ],
          "includeContext": "thisServer",
          "metadata": {
            "customKey": "customValue"
          }
        }
      }
    },
    {
      "description": "llama-70b generated negative",
      "valid": false,
      "rust_error": "\"invalidContext\" is not one of [\"allServers\",\"none\",\"thisServer\"]",
      "python_error": "'invalidContext' is not one of ['allServers', 'none', 'thisServer']\n\nFailed validating 'enum' in schema['properties']['params']['properties']['includeContext']:\n    {'description': 'A request to include context from one or more MCP '\n                    'servers (including the caller), to be attached to the '\n                    'prompt. The client MAY ignore this request.',\n     'enum': ['allServers', 'none', 'thisServer'],\n     'type': 'string'}\n\nOn instance['params']['includeContext']:\n    'invalidContext'",
      "data": {
        "method": "sampling/createMessage",
        "params": {
          "maxTokens": -1,
          "messages": [
            {
              "content": {
                "type": "text",
                "text": "Hello, how are you?",
                "annotations": {
                  "priority": 2,
                  "audience": [
                    "assistant",
                    "user"
                  ]
                }
              },
              "role": "assistant"
            },
            {
              "content": {
                "type": "text",
                "text": "I'm good, thanks!",
                "annotations": {
                  "priority": 1,
                  "audience": [
                    "assistant"
                  ]
                }
              },
              "role": "user"
            }
          ],
          "modelPreferences": {
            "intelligencePriority": 2,
            "costPriority": 0.2,
            "speedPriority": 0.5,
            "hints": [
              {
                "name": "claude-3-5-sonnet"
              }
            ]
          },
          "temperature": 2,
          "systemPrompt": "Please respond with a haiku.",
          "stopSequences": [
            "\n",
            "\n\n"
          ],
          "includeContext": "invalidContext",
          "metadata": {
            "customKey": "customValue"
          }
        }
      }
    }
  ]
}