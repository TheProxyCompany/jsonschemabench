#!/usr/bin/env python3

import json
import math
import glob
import sys
import os
import re

import matplotlib.pyplot as plt
import numpy as np

class Stats:
    def __init__(self) -> None:
        self.meta = {}
        self.ttfm_us = 0
        self.max_ttfm_us = 0
        self.masks_us = 0
        self.avg_mask_us = 0
        self.masks_us_under_10ms = 0
        self.num_masks_under_10ms = 0
        self.avg_masks_under_10ms = 0
        self.masks_us_over_10ms = 0
        self.num_masks_over_10ms = 0
        self.avg_masks_over_10ms = 0
        self.max_mask_us = 0
        self.num_tokens = 0
        self.num_schemas = 0
        self.num_crashes_or_timeouts = 0
        self.num_timeouts = 0
        self.num_segv = 0
        self.num_abort = 0
        self.num_schemas_ok = 0
        self.num_compilation_errors = 0
        self.num_validation_errors = 0
        self.num_invalidation_errors = 0
        self.num_tests = 0
        self.num_valid_tests = 0
        self.num_invalid_tests = 0

    def load_json(self, data: dict):
        for k in self.__dict__.keys():
            if k in data:
                self.__dict__[k] = data[k]


def log_fraction_plot(times: list[int]):
    times.sort()
    cutoff = 1
    mult = 1.3
    count = 0
    csv = "cutoff time,frac left\n"
    total = len(times)
    for t in times:
        while t > cutoff:
            csv += f"{cutoff/1000.0},{(total - count)/total}\n"
            cutoff = int(cutoff * mult) + 1
        count += 1
    return csv


def histogram_position(us: int):
    return int(math.floor(math.log10(max(1, us - 1))))


def us_to_str(us: int):
    if us < 1000:
        return f"{us}us"
    if us < 1000000:
        return f"{us//1000}ms"
    return f"{us//1000000}s"


def read_json(filename: str):
    with open(filename) as f:
        data = json.load(f)
    return data


def main(folder: str):
    # for llg rust: "tmp/llg_results.json"
    if not os.path.isdir(folder):
        raise Exception(f"Not a directory: {folder}")
    files = glob.glob(folder + "/*.json")
    files = sorted(files)

    stats = Stats()
    stats.meta = read_json(folder + "/meta.txt")
    ttfm_us = []
    all_masks_us = []
    histogram_us = [0] * 10
    histogram_num = [0] * 10
    for f in files:
        with open(f) as f:
            data = json.load(f)
        elts = [data]
        if isinstance(data, list):
            elts = data
        for data in elts:
            stats.num_schemas += 1
            if "num_tests" not in data:
                stats.num_crashes_or_timeouts += 1
                continue
            stats.num_tests += data["num_tests"]
            if "compile_error" in data:
                stats.num_compilation_errors += 1
            else:
                stats.ttfm_us += data["ttfm_us"]
                ttfm_us.append(data["ttfm_us"])
                stats.max_ttfm_us = max(data["max_ttfm_us"], stats.max_ttfm_us)
                if "masks_us" in data:
                    stats.masks_us += data["masks_us"]
                    stats.max_mask_us = max(data["max_mask_us"], stats.max_mask_us)
                    stats.num_tokens += data["num_tokens"]
                if "validation_error" in data:
                    if "should reject" in data["validation_error"]:
                        stats.num_invalidation_errors += 1
                    else:
                        stats.num_validation_errors += 1
                else:
                    stats.num_schemas_ok += 1
                stats.num_valid_tests += data["num_valid_tests"]
                stats.num_invalid_tests += data["num_invalid_tests"]
                all_masks_us.extend(data["all_mask_us"])
                for us in data["all_mask_us"]:
                    p = histogram_position(us)
                    histogram_us[p] += us
                    histogram_num[p] += 1
                    if us < 10000:
                        stats.masks_us_under_10ms += us
                        stats.num_masks_under_10ms += 1
                    else:
                        stats.masks_us_over_10ms += us
                        stats.num_masks_over_10ms += 1
    stats.avg_masks_under_10ms = stats.masks_us_under_10ms // stats.num_masks_under_10ms
    stats.avg_masks_over_10ms = stats.masks_us_over_10ms // stats.num_masks_over_10ms
    stats.avg_mask_us = stats.masks_us // stats.num_tokens
    print(json.dumps(stats.__dict__, indent=2))
    with open(folder + "/stats.txt", "w") as f:
        f.write(json.dumps(stats.__dict__, indent=2))
    with open(folder + "/ttfm_us.csv", "w") as f:
        f.write(log_fraction_plot(ttfm_us))
    with open(folder + "/masks_us.csv", "w") as f:
        f.write(log_fraction_plot(all_masks_us))

    output_file = os.path.join(folder, "log.txt")
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    try:
        with open(output_file, "r") as f:
            lines = f.readlines()
            for line in lines:
                line = line.strip()
                if line == "Exit code: -11":
                    stats.num_segv += 1
                elif line == "Exit code: -6":
                    stats.num_abort += 1
                elif line == "Exit code: -14":
                    stats.num_timeouts += 1
    except FileNotFoundError:
        # File doesn't exist yet, create an empty one
        with open(output_file, "w") as f:
            pass

    ps = [25, 50, 75, 90, 95, 99, 99.9, 100]

    def get_p(arr: list[int], p: float):
        idx = int(len(arr) * p / 100)
        if idx >= len(arr):
            idx = len(arr) - 1
        return arr[idx]

    entries = {}
    all_masks_us.sort()
    entries["TBM avg"] = sum(all_masks_us) // len(all_masks_us)
    for p in ps:
        entries[f"TBM p{p}"] = get_p(all_masks_us, p)
    ttfm_us += [900_000_000] * stats.num_timeouts
    ttfm_us.sort()
    entries["TTFM avg"] = sum(ttfm_us) // len(ttfm_us)
    for p in ps:
        entries[f"TTFM p{p}"] = get_p(ttfm_us, p)
    entries["tokens"] = stats.num_tokens
    entries["schemas"] = stats.num_schemas
    entries["passing"] = stats.num_schemas_ok
    # entries["crashes"] = stats.num_crashes_or_timeouts
    entries["compile error"] = stats.num_compilation_errors
    entries["segmentation fault"] = stats.num_segv
    entries["out of memory"] = stats.num_abort
    entries["timeout"] = stats.num_timeouts
    entries["validation error"] = stats.num_validation_errors
    entries["invalidation error"] = stats.num_invalidation_errors
    print(json.dumps(entries, indent=2))
    with open(folder + "/entries.txt", "w") as f:
        f.write(json.dumps(entries, indent=2))
    # print(f"{'ttfm_p' + str(p):10}, {ttfm_us[int(len(ttfm_us) * p / 100)]}")

    num_masks = sum(histogram_num)
    h_csv = "above us,frac\n"
    for i in range(10)[1:]:
        frac = sum(histogram_num[i:]) * 100 / num_masks
        h_csv += f"{us_to_str(10**i):10}"
        h_csv += ","
        h_csv += f"{frac:1.15}"
        h_csv += "\n"
    with open(folder + "/histogram.csv", "w") as f:
        f.write(h_csv)
    # print(h_csv)

    return stats, entries


def markdown_table(data):
    # Determine the column widths
    col_widths = [max(len(row[col]) for row in data) for col in range(len(data[0]))]

    # Generate the Markdown table
    def format_row(row):
        return (
            "| "
            + " | ".join(
                f"{cell:<{col_widths[i]}}" if i == 0 else f"{cell:>{col_widths[i]}}"
                for i, cell in enumerate(row)
            )
            + " |\n"
        )

    r = format_row(data[0])
    r += (
        "|"
        + "|".join(
            ":" + "-" * (width + 1) if i == 0 else "-" * (width + 1) + ":"
            for i, width in enumerate(col_widths)
        )
        + "|\n"
    )

    # Print the rows
    for row in data[1:]:
        r += format_row(row)

    return r


def format_time(time_us: int):
    if time_us < 1_000:
        result = f"{time_us}Î¼s"
    elif time_us < 1_000_000:
        time_ms = time_us / 1_000
        if time_ms < 10:
            result = f"{time_ms:.1f}ms"
        else:
            result = f"{time_ms:.0f}ms"
    else:
        time_s = time_us / 1_000_000
        if time_s < 10:
            result = f"{time_s:.1f}s"
        else:
            result = f"{time_s:.0f}s"
    return result


plot_colors = {
    "llg": "#000000",
    "xgr": 9,
    "llamacpp": 1,
    "xgr-cpp": 0,
    "outlines": 2,
    "pse": "#024645",
}


def plot_metrics(data_list: list[dict], id: str, keys: list[str], title: str):
    labels = [data["meta"]["name"] for data in data_list]
    values = {
        label: [data[key] for key in keys] for label, data in zip(labels, data_list)
    }

    # Calculate positions
    y = np.arange(len(keys))
    height = 0.8 / len(labels)

    # Set up the plot
    fig, ax = plt.subplots(figsize=(10, len(keys) + 1))
    bar_positions = []

    cmap = plt.get_cmap("tab10")

    custom_colors = [
        "#000000",
        "#0379b6",
        "#039eb6",
        "#d16626",
    ]

    custom_colors = []
    for c in plot_colors.values():
        if isinstance(c, int):
            custom_colors.append(cmap(c))
        else:
            custom_colors.append(c)

    for i, (label, tbm_values) in enumerate(values.items()):
        pos = y + (len(labels) - i - 1) * height - 0.4 + (height / 2)
        bar_positions.append(pos)
        bars = ax.barh(
            pos,
            tbm_values,
            height,
            label=label,
            color=custom_colors[i % len(custom_colors)],
        )

        # Add annotations on the bars
        for j, (bar, value) in enumerate(zip(bars, tbm_values)):
            label_text = format_time(value)
            ax.text(
                bar.get_width() * 1.1,
                bar.get_y() + bar.get_height() / 2,
                label_text,
                ha="left",
                va="center",
                fontsize=8,
                rotation=0,
            )

            # Add speedup/slowdown factor
            all_values_for_key = [values[engine][j] for engine in labels]
            fastest = min(all_values_for_key)
            factor = value / fastest

            speedup_text = f"{factor:.0f}x"
            ax.text(
                bar.get_width() / 1.2,
                bar.get_y() + bar.get_height() / 2,
                speedup_text,
                ha="right",
                va="center",
                fontsize=8,
                rotation=0,
                color="white",
            )

    # Configure axes
    ax.set_xscale("log", base=10)
    ax.set_yticks(y)
    ax.set_yticklabels(keys, rotation=45, ha="right")
    ax.set_xlabel("Time (log scale, microseconds)")
    ax.set_title(title)
    ax.legend()

    plt.tight_layout()
    plt.savefig(f"plots/{id}.png", dpi=300)


if __name__ == "__main__":
    folders = sys.argv[1:]
    quick = False
    if len(folders) > 0 and folders[0] == "-q":
        quick = True
        folders = folders[1:]

    selective = True
    if not folders:
        selective = False
        metas = glob.glob("tmp/out--*/meta.txt")
        folders = [os.path.dirname(m) for m in metas]

    ents: list[dict] = []
    all_stats: list[Stats] = []
    hd = ["metric"]
    breakpoint()
    for fld in folders:
        if quick:
            stats = Stats()
            stats.load_json(read_json(fld + "/stats.txt"))
            entries = read_json(fld + "/entries.txt")
        else:
            stats, entries = main(fld)
        entries["meta"] = stats.meta
        ents.append(entries)
        all_stats.append(stats)

    positions = list(plot_colors.keys())
    ents.sort(key=lambda e: positions.index(e["meta"]["id"]))

    hd += [e["meta"]["name"] for e in ents]
    rows = [hd]
    for k in ents[0].keys():
        if k == "meta":
            continue
        line = [k]
        for e in ents:
            line.append(f"{e[k]:,}")
        rows.append(line)
    tbl = markdown_table(rows)

    tbl += "\n\n### Versions\n\n"
    seen_modules = set()
    for st in ents:
        meta = st["meta"]
        module = meta["module"]
        if module in seen_modules:
            continue
        seen_modules.add(module)
        tbl += f"* {module}: {meta['module_version']}\n"

    print(tbl)

    if selective:
        sys.exit(0)

    with open("README.md", "r") as f:
        rdm = f.read()
        rdm = re.sub(
            r"<!-- GEN-BEGIN -->.*?<!-- GEN-END -->",
            "<!-- GEN-BEGIN -->\n" + tbl + "<!-- GEN-END -->",
            rdm,
            flags=re.DOTALL,
        )
    with open("README.md", "w") as f:
        f.write(rdm)

    keys = ents[0].keys()

    plot_metrics(
        ents,
        keys=[k for k in keys if k.startswith("TBM ")],
        id="tbm",
        title="Per-token mask computation time (Time Between Masks aka TBM)",
    )
    plot_metrics(
        ents,
        keys=[k for k in keys if k.startswith("TTFM ")],
        id="ttfm",
        title="Grammar compilation (Time To First Mask aka TTFM); 900s timeout",
    )
    plot_metrics(
        ents,
        keys=["TTFM avg", "TBM avg"],
        id="hero",
        title="Time To First Mask (TTFM) and Time Between Masks (TBM)",
    )
